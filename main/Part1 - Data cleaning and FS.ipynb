{"cells":[{"cell_type":"markdown","id":"a1a7a248-220a-4933-a12c-db48c6e4bf08","metadata":{},"source":["## Tweet project - Part 1\n","\n","#### 1. Load data\n","#### 2. Select relevant tweets\n","#### 3. Select important features to be used in model\n","#### 4. Save the filtered data in parquet format for later use"]},{"cell_type":"code","execution_count":1,"id":"816e0738-00f1-41e2-8000-2da77fc2dcb2","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n","[GCC 10.3.0]\n","3.1.3\n"]}],"source":["import sys\n","print(sys.version)\n","print(spark.version)"]},{"cell_type":"code","execution_count":2,"id":"bafcf964-892c-4924-afbb-51f1ff139ce9","metadata":{},"outputs":[],"source":["import os\n","import time\n","import subprocess\n","\n","from pyspark.sql.functions import *\n","from pyspark.sql.types import *\n","\n","from pyspark.sql import SQLContext\n","sqlContext = SQLContext(sc)"]},{"cell_type":"code","execution_count":3,"id":"1912c400-be01-4437-9e64-562cac01305f","metadata":{},"outputs":[],"source":["warnings.filterwarnings(action='ignore')\n","spark = SparkSession.builder.getOrCreate()\n","\n","##Add \"eagerEval.enabled\" to beautify the way Spark DF is displayed\n","spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\n","\n","## To use legacy casting notation for date\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")"]},{"cell_type":"markdown","id":"e98757aa-d70e-48fc-b447-472df112254c","metadata":{},"source":["## 1. Load data\n","\n","### 1.1 Check total data volume in bucket. Should be around ~500 GB"]},{"cell_type":"code","execution_count":4,"id":"5f066ef2-e474-4970-bc19-d27af147ee7f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["497.0 G  497.0 G  gs://msca-bdp-tweets/final_project\n","\n"]}],"source":["## This data was downloaded using the twitter API and stored in a personal bucket on Google Cloud\n","\n","bucket_name = 'prayutjain-tweet-bucket' # Personal dir\n","prefix = 'tweet_project'\n","\n","cmd = 'hadoop fs -du -s -h ' + 'gs://' + bucket_name + '/' + prefix + '/'\n","\n","p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n","for line in p.stdout.readlines():\n","    print (line)\n","    \n","retval = p.wait()"]},{"cell_type":"code","execution_count":5,"id":"3bb57ac5-7fd9-4dd9-bfdb-54551fbbf455","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 36976 items\n","-rwx------   3 root root          0 2022-11-06 23:17 gs://msca-bdp-tweets/final_project/_SUCCESS\n","-rwx------   3 root root    6007332 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00000-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    5638649 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00001-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    6649652 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00002-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    6921001 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00003-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    6626757 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00004-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    6696245 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00005-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    6658398 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00006-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n","-rwx------   3 root root    4963067 2022-11-06 23:12 gs://msca-bdp-tweets/final_project/part-00007-f654a635-796b-4190-88ae-6c2ee7e6f3a3-c000.json\n"]}],"source":["# !hadoop fs -ls 'gs://prayutjain-tweet-bucket/tweet_project/' | head "]},{"cell_type":"markdown","id":"3bd0cd15-d79c-43c9-93b2-f97fe12a9314","metadata":{},"source":["### 1.2 Read .json files"]},{"cell_type":"code","execution_count":null,"id":"b187e0fd-2006-4bfd-831f-55aa8ca44bfa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/05 20:27:55 WARN org.apache.spark.sql.execution.datasources.SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["CPU times: user 1.26 s, sys: 259 ms, total: 1.52 s\n","Wall time: 6min 38s\n"]},{"name":"stderr","output_type":"stream","text":["22/12/05 20:33:27 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"]}],"source":["%time tweets_raw = spark.read.json('gs://' + bucket_name + '/' + prefix)\n"]},{"cell_type":"markdown","id":"8d838eeb-b6c3-4c80-b370-ed54f89af69e","metadata":{},"source":["## 2. Select education relevant tweets\n","\n","### 2.1 Filter tweets for \"K-12/education\" related topics"]},{"cell_type":"code","execution_count":6,"id":"d404c303-ccb7-4b38-aba2-6ad34d386d07","metadata":{},"outputs":[],"source":["## Stripped text - all low case, broken into words \n","\n","tweets_raw = tweets_raw\\\n",".withColumn('tweet_text', lower('tweet_text'))\\\n",".withColumn('stripped', regexp_replace(col(\"tweet_text\"),\"[\\$#,&%\\\".]\",\"\"))\n","#withColumn('match_case', array(filt_words1))\n","#withColumn('stripped', split('stripped', ' '))\n"]},{"cell_type":"code","execution_count":7,"id":"fd612c0f-aa92-4460-8d21-5273ca040f2d","metadata":{},"outputs":[],"source":["## Dictionary for words similar to 'education/K-12' \n","\n","dict_filt = [\"primary school\",\"schools\",\"education\",\"k12\",\"high school\",\"teacher\",\"higher secondary\",\n","             \"senior secondary\",\"sophomore\",\"math\",\"mathematics\",\"science\",\"physics\",\"chemistry\",\"biology\",\"humanities\",\n","             \"history\",\"philosophy\",\"alma mater\",\"academia\",\"educating\",\"teaching\",\"curriculum\",\"online learning\",\n","             \"educational\",\"textbook\",\"kindergarten\",\"schooling\",\"k-12\",\"social-emotional learning\",\"training\",\"knowledge\",\n","             \"scholarship\",\"literacy\",\"schooling\",\"tuition\",\"undergraduate\",\"academic\",\"course\",\"graduate\",\"stem\",\"phd\",\n","             \"classwork\",\"classroom\",\"preschool\",\"educationist\",\"syllabus\",\"middle school\",\"secondary school\",\"undergrad\"]\n","\n","dict_filt1='|'.join([\"(\" + c +\")\" for c in dict_filt])\n","\n","dict_rm = [\"shoot\",\"kill\",\"killed\",\"deceased\",\"murder\",\"attack\",\"horny\",\n","           \"shooting\",\"shootings\",\"gunned\",\"gun\",\"guns\",\"uvalde\"]\n","\n","dict_rm1='|'.join([\"(\" + c +\")\" for c in dict_rm])\n","\n","tweets_filt = tweets_raw.where(tweets_raw['tweet_text'].rlike(dict_filt1)).\\\n","where(~tweets_raw['tweet_text'].rlike(dict_rm1))"]},{"cell_type":"code","execution_count":null,"id":"8976719a-1c10-4102-bbcb-0f3b025a6a6f","metadata":{},"outputs":[],"source":["## Check which words are selecting the most filtered tweets\n","res = []\n","for keys in dict_filt:\n","    filter_string = 'tweet_text like \"%' + keys + '%\"'\n","    temp = tweets_filt.filter(filter_string).count()\n","    res.append([temp, keys])\n","    \n","sorted(res,key=lambda l:l[0], reverse = True)[:20]"]},{"cell_type":"code","execution_count":null,"id":"55315f46-1b8e-4273-a257-722a63ea85fd","metadata":{"scrolled":true,"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/05 21:34:42 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000067 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.221]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.221]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.224]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000066 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.228]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.229]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.231]Killed by external signal\n",".\n","22/12/05 21:34:42 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 65 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000067 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.221]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.221]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.224]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.deploy.yarn.YarnAllocator: Cannot find executorId for container: container_1670262909868_0003_01_000072\n","22/12/05 21:34:42 WARN org.apache.spark.deploy.yarn.YarnAllocator: Cannot find executorId for container: container_1670262909868_0003_01_000073\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 65 for reason Container from a bad node: container_1670262909868_0003_01_000067 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.221]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.221]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.224]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 64 for reason Container from a bad node: container_1670262909868_0003_01_000066 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.228]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.229]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.231]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4374.0 in stage 5.0 (TID 30602) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal executor 65): ExecutorLostFailure (executor 65 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000067 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.221]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.221]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.224]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4385.0 in stage 5.0 (TID 30613) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal executor 65): ExecutorLostFailure (executor 65 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000067 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.221]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.221]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.224]Killed by external signal\n",".\n","22/12/05 21:34:42 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 64 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000066 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.228]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.229]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.231]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4380.0 in stage 5.0 (TID 30608) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal executor 64): ExecutorLostFailure (executor 64 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000066 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.228]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.229]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.231]Killed by external signal\n",".\n","22/12/05 21:34:42 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 4388.0 in stage 5.0 (TID 30616) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal executor 64): ExecutorLostFailure (executor 64 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000066 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 21:34:42.228]Container killed on request. Exit code is 143\n","[2022-12-05 21:34:42.229]Container exited with a non-zero exit code 143. \n","[2022-12-05 21:34:42.231]Killed by external signal\n",".\n","22/12/05 21:36:27 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 7.0 (TID 31633) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal executor 67): FetchFailed(BlockManagerId(64, hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal, 7337, None), shuffleId=0, mapIndex=8, mapId=26236, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=1028232886000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=64)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=1028232886000,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=64)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","22/12/05 21:37:06 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 7.1 (TID 32089) (hub-msca-bdp-dphub-students-backup-prayutjain-w-1.c.msca-bdp-students.internal executor 66): FetchFailed(BlockManagerId(65, hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal, 7337, None), shuffleId=0, mapIndex=18, mapId=26246, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=1028232886002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=65)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=1028232886002,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=65)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-----------+\n","|before_filt|\n","+-----------+\n","|   99992797|\n","+-----------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["22/12/05 21:38:08 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 62 for reason Container marked as failed: container_1670262909868_0003_01_000064 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:38:08 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 63 for reason Container marked as failed: container_1670262909868_0003_01_000065 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:38:08 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 63 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000065 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:38:08 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 62 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000064 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:39:58 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 70 for reason Container marked as failed: container_1670262909868_0003_01_000074 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:39:58 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 71 for reason Container marked as failed: container_1670262909868_0003_01_000075 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:39:58 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 70 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000074 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:39:58 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 71 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000075 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 22:05:24 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 10.0 (TID 37952) (hub-msca-bdp-dphub-students-backup-prayutjain-w-1.c.msca-bdp-students.internal executor 66): FetchFailed(BlockManagerId(62, hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=8, mapId=32551, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=465411266001,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=62)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=465411266001,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=62)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","22/12/05 22:05:29 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 10.1 (TID 37963) (hub-msca-bdp-dphub-students-backup-prayutjain-w-1.c.msca-bdp-students.internal executor 66): FetchFailed(BlockManagerId(71, hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=6, mapId=32549, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=514219536003,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=71)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=514219536003,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=71)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","22/12/05 22:05:46 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 10.2 (TID 38009) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal executor 69): FetchFailed(BlockManagerId(63, hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=7, mapId=32550, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=465411266008,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=63)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=465411266008,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=63)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","22/12/05 22:05:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 10.3 (TID 38020) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal executor 67): FetchFailed(BlockManagerId(70, hub-msca-bdp-dphub-students-backup-prayutjain-sw-w6z6.c.msca-bdp-students.internal, 7337, None), shuffleId=1, mapIndex=5, mapId=32548, reduceId=0, message=\n","org.apache.spark.shuffle.FetchFailedException: Failure while fetching StreamChunkId[streamId=514219536008,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=70)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:775)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:690)\n","\tat org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:70)\n","\tat org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)\n","\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n","\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n","\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey_0$(Unknown Source)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:505)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:508)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.network.client.ChunkFetchFailureException: Failure while fetching StreamChunkId[streamId=514219536008,chunkIndex=0]: java.lang.RuntimeException: Executor is not registered (appId=application_1670262909868_0003, execId=70)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getContinuousBlocksData(ExternalShuffleBlockResolver.java:188)\n","\tat org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:170)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:479)\n","\tat org.apache.spark.network.shuffle.ExternalBlockHandler$ShuffleManagedBufferIterator.next(ExternalBlockHandler.java:445)\n","\tat org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:87)\n","\tat org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:103)\n","\tat org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:107)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:140)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat org.sparkproject.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat org.sparkproject.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\tat org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:182)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)\n","\tat io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\t... 1 more\n","\n",")\n","[Stage 9:=======================================================> (45 + 1) / 46]\r"]},{"name":"stdout","output_type":"stream","text":["+----------+\n","|after_filt|\n","+----------+\n","|  35186761|\n","+----------+\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["tweets_raw.select(count('*').alias('before_filt')).show()\n","tweets_filt.select(count('*').alias('after_filt')).show()"]},{"cell_type":"markdown","id":"9a167019-4f56-46c3-a936-e8666d25736b","metadata":{},"source":["## 3. Feature selection\n","\n","### 3.1 Check data coverage"]},{"cell_type":"code","execution_count":null,"id":"e867db34-9339-40e7-b652-b0729fd7f963","metadata":{},"outputs":[],"source":["## Parent field coverage - low for some field: coordinates, extended_entities, geo, place, quoted_status_* .... \n","\n","tweets_filt.select([(count(when(col(c).isNull(), c))/count(lit(1))).alias(c) for c in tweets_filt.schema.names]).show(truncate=True)"]},{"cell_type":"markdown","id":"f82b7394-4b9a-4e1e-9cf1-ac64851e2d16","metadata":{},"source":["### 3.2 Select only the well populated cols"]},{"cell_type":"code","execution_count":9,"id":"a852382e-edd4-49e6-bfe9-95fbe16e739c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+-------------------+--------------------+---------------------+--------------------+------------------+-------------------+--------------------+--------------+----------------+-------------------+-------------+--------------------------+--------------------+-----------------+-------------------+-------------------+----------+------------------------+----------------------+--------------------+------------------+-------------------+--------------------+--------+-------------+--------------+-----------+-----------+\n","|    user_created_at|    user_description|user_favourites_count|user_followers_count|user_friends_count|        user_id_str|           user_name|user_protected|user_screen_name|user_statuses_count|user_verified|user_withheld_in_countries|       user_location|tweet_coordinates|   tweet_created_at|       tweet_id_str|tweet_lang|tweet_possibly_sensitive|tweet_retweeted_status|    tweet_tweet_text|tweet_timestamp_ms|tweet_quoted_status|          tweet_text|hashtags|retweet_count|favorite_count|reply_count|quote_count|\n","+-------------------+--------------------+---------------------+--------------------+------------------+-------------------+--------------------+--------------+----------------+-------------------+-------------+--------------------------+--------------------+-----------------+-------------------+-------------------+----------+------------------------+----------------------+--------------------+------------------+-------------------+--------------------+--------+-------------+--------------+-----------+-----------+\n","|2022-05-19 00:00:53|                null|                    0|                   0|                31|1527076684557451264|  High School Sports|         false| Gabriel50407921|                160|        false|                        []|                null|             null|2022-05-24 22:09:56|1529223197253435392|        en|                   false|                  null|indiana high scho...|     1653430196475|               null|Indiana High Scho...|      []|         null|          null|       null|       null|\n","|2012-03-19 02:44:38|SHSU ALUMNA | ΣΓΡ...|                 2158|                 327|               646|          529043291|Salvadorean Poodl...|         false|       Lizzy9839|              11657|        false|                        []|                null|             null|2022-05-24 22:09:56|1529223199036059648|        en|                    null|  {null, Mon May 23...|my son was not al...|     1653430196900|               null|RT @meganbang3: M...|      []|        23904|        308428|       5119|       1625|\n","|2010-12-31 18:44:15|The ability to sp...|               222404|                1763|               926|          232629239|              Hakeem|         false|      Hakeem_jnr|             282270|        false|                        []|Old Trafford / AT...|             null|2022-05-24 22:09:58|1529223207072706560|        en|                    null|  {null, Tue May 24...|there simply has ...|     1653430198816|               null|RT @KingJames: Th...|      []|         8655|         65196|        808|        266|\n","|2009-02-19 04:54:16|Owner @ajarproduc...|               146347|                 975|              1820|           21277152|Justin Putney #Bl...|         false|    justinputney|              22973|        false|                        []|San Francisco Bay...|             null|2022-05-24 22:09:58|1529223207164649473|        en|                    null|  {null, Tue May 24...|florida high scho...|     1653430198838|               null|RT @mattxiv: flor...|      []|        10238|         50016|        418|       1503|\n","|2018-11-27 22:48:43|               achoo|                19293|                  58|                90|1067550814904844288|             sneezus|         false|     peepeeegirl|               2709|        false|                        []|             kleenex|             null|2022-05-24 22:09:59|1529223210675212288|        en|                    null|                  null|since it's about ...|     1653430199675|               null|since it's about ...|      []|         null|          null|       null|       null|\n","+-------------------+--------------------+---------------------+--------------------+------------------+-------------------+--------------------+--------------+----------------+-------------------+-------------+--------------------------+--------------------+-----------------+-------------------+-------------------+----------+------------------------+----------------------+--------------------+------------------+-------------------+--------------------+--------+-------------+--------------+-----------+-----------+\n","only showing top 5 rows\n","\n"]}],"source":["tweet_cols = [\"coordinates\",\"created_at\",\"id_str\",\"lang\",\"possibly_sensitive\",\"retweeted_status\",\n","              \"tweet_text\",\"timestamp_ms\",\"quoted_status\",\"text\"]\n","\n","user_cols = [\"created_at\",\"description\",\"favourites_count\",\"followers_count\",\"friends_count\",\"id_str\",\n","            \"name\",\"protected\",\"screen_name\",\"statuses_count\",\"verified\",\"withheld_in_countries\",\"location\"]\n","\n","ent_cols = [\"hashtags\"]\n","\n","retweet_cols = [\"retweet_count\",\"favorite_count\",\"reply_count\",\"quote_count\"]\n","\n","quoted_cols = [\"quote_count\"]\n","\n","df = tweets_filt.select([*[col('user.' + col_name).alias('user_' + col_name) for col_name in user_cols],\n","                                  *[col(col_name).alias('tweet_' + col_name) for col_name in tweet_cols],\n","                                  *[col('entities.' + col_name).alias(col_name) for col_name in ent_cols],\n","                            *[col('retweeted_status.' + col_name).alias(col_name) for col_name in retweet_cols]])\\\n",".withColumn('user_created_at',to_timestamp(col('user_created_at'),'EEE MMM dd HH:mm:ss zzzzz yyyy'))\\\n",".withColumn('tweet_created_at',to_timestamp(col('tweet_created_at'),'EEE MMM dd HH:mm:ss zzzzz yyyy'))\n","\n","df.show(5, truncate=True)"]},{"cell_type":"markdown","id":"6d14f980-f344-475a-a79e-df02b368dc86","metadata":{},"source":["## 4. Save filtered and selected features data"]},{"cell_type":"code","execution_count":null,"id":"f988023e-d5d3-4595-bb51-cc9afa34eb53","metadata":{"scrolled":true,"tags":[]},"outputs":[{"name":"stderr","output_type":"stream","text":["22/12/05 20:35:29 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 35 for reason Container marked as failed: container_1670262909868_0003_01_000035 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:35:29 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 31 for reason Container marked as failed: container_1670262909868_0003_01_000031 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:36:13 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000043 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.406]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.407]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.413]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000036 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.438]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.438]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.441]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 43 for reason Container from a bad node: container_1670262909868_0003_01_000043 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.406]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.407]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.413]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 36 for reason Container from a bad node: container_1670262909868_0003_01_000036 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.438]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.438]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.441]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.deploy.yarn.YarnAllocator: Cannot find executorId for container: container_1670262909868_0003_01_000047\n","22/12/05 20:36:13 WARN org.apache.spark.deploy.yarn.YarnAllocator: Cannot find executorId for container: container_1670262909868_0003_01_000048\n","22/12/05 20:36:13 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 43 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000043 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.406]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.407]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.413]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 53.0 in stage 3.0 (TID 15455) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal executor 43): ExecutorLostFailure (executor 43 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000043 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.406]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.407]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.413]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 52.0 in stage 3.0 (TID 15454) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal executor 43): ExecutorLostFailure (executor 43 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000043 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.406]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.407]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.413]Killed by external signal\n",".\n","22/12/05 20:36:13 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 36 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000036 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.438]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.438]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.441]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 50.0 in stage 3.0 (TID 15452) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal executor 36): ExecutorLostFailure (executor 36 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000036 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.438]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.438]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.441]Killed by external signal\n",".\n","22/12/05 20:36:13 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 94.0 in stage 3.0 (TID 15496) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal executor 36): ExecutorLostFailure (executor 36 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000036 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-587z.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:13.438]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:13.438]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:13.441]Killed by external signal\n",".\n","22/12/05 20:36:16 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 39 for reason Container marked as failed: container_1670262909868_0003_01_000039 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:36:16 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 32 for reason Container marked as failed: container_1670262909868_0003_01_000032 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:36:16 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 39 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000039 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:36:16 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 32 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000032 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:36:20 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000033 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.667]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.667]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.668]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1670262909868_0003_01_000029 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.668]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.668]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.670]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 33 for reason Container from a bad node: container_1670262909868_0003_01_000033 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.667]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.667]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.668]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 29 for reason Container from a bad node: container_1670262909868_0003_01_000029 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.668]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.668]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.670]Killed by external signal\n",".\n","22/12/05 20:36:20 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 33 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000033 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.667]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.667]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.668]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 105.0 in stage 3.0 (TID 15515) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal executor 33): ExecutorLostFailure (executor 33 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000033 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.667]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.667]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.668]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 109.0 in stage 3.0 (TID 15519) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal executor 33): ExecutorLostFailure (executor 33 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000033 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.667]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.667]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.668]Killed by external signal\n",".\n","22/12/05 20:36:20 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 29 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal: Container from a bad node: container_1670262909868_0003_01_000029 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.668]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.668]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.670]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 104.0 in stage 3.0 (TID 15514) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal executor 29): ExecutorLostFailure (executor 29 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000029 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.668]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.668]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.670]Killed by external signal\n",".\n","22/12/05 20:36:20 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 72.1 in stage 3.0 (TID 15511) (hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal executor 29): ExecutorLostFailure (executor 29 exited caused by one of the running tasks) Reason: Container from a bad node: container_1670262909868_0003_01_000029 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: 143. Diagnostics: [2022-12-05 20:36:20.668]Container killed on request. Exit code is 143\n","[2022-12-05 20:36:20.668]Container exited with a non-zero exit code 143. \n","[2022-12-05 20:36:20.670]Killed by external signal\n",".\n","22/12/05 20:40:05 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 47 for reason Container marked as failed: container_1670262909868_0003_01_000049 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:40:05 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 48 for reason Container marked as failed: container_1670262909868_0003_01_000050 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:40:05 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 48 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000050 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:40:05 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 47 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000049 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wd0p.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:58:27 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 49 for reason Container marked as failed: container_1670262909868_0003_01_000051 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:58:27 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 50 for reason Container marked as failed: container_1670262909868_0003_01_000052 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:58:27 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 49 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000051 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 20:58:27 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 50 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000052 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-wpsr.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:06:06 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 53 for reason Container marked as failed: container_1670262909868_0003_01_000055 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:06:06 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 54 for reason Container marked as failed: container_1670262909868_0003_01_000056 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:06:06 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 54 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000056 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","22/12/05 21:06:06 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 53 on hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal: Container marked as failed: container_1670262909868_0003_01_000055 on host: hub-msca-bdp-dphub-students-backup-prayutjain-sw-vc4k.c.msca-bdp-students.internal. Exit status: -100. Diagnostics: Container released on a *lost* node.\n","                                                                                \r"]}],"source":["tweets_filt.write.format(\"parquet\").\\\n","mode('overwrite').\\\n","save('gs://prayutjain-tweet-bucket/filtered_data')\n","\n","df.write.format(\"parquet\").\\\n","mode('overwrite').\\\n","save('gs://prayutjain-tweet-bucket/processed_data')"]},{"cell_type":"code","execution_count":null,"id":"571a9096-817c-4119-8e67-fdfa64cf02dc","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"6a901947c0cb8364e0dfa0f2a25df49fda9dbe95863cc875df85a4829f575e4b"}}},"nbformat":4,"nbformat_minor":5}
